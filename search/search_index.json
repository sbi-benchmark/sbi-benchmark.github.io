{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Benchmarking Simulation-Based Inference Many domains of science make use of numerical simulators for which (1) it is easy to simulate but (2) the likelihood is unavailable, making it hard to perform statistical identification of model parameters consistent with observed data. Simulation-based inference (SBI) deals with this 'likelihood-free' setting. Although recent advances have led to a large number of SBI algorithms, a public benchmark for such algorithms has been lacking: We set out to fill this gap, carefully select tasks and metrics, and evaluate several canonical algorithms. Through this website you can explore all results of our manuscript , including comparisons on all metrics and plotting of posteriors for each of more than 10 000 runs. Keep reading for a brief summary of the manuscript, or jump right into interactive results (through the menu on top). We provide a framework to benchmark your own algorithms, code and results for reproducibility, and invite contributions. Our Motivation and Methods Open benchmarks can be an important component of transparent and reproducible computational research. However, a benchmark framework for SBI has been lacking, possibly due to the challenging endeavour of designing benchmarking tasks and defining suitable performance metrics. We selected a set of initial algorithms representing four distinct approaches to SBI, analyzed multiple performance metrics which have been used in the literature, and implemented ten tasks, including ones popular in the field. Overview of algorithms. Classification and schemes following Cranmer et al. (2020) . Algorithms . We compare algorithms belonging to four distinct approaches to SBI: Classical ABC approaches as well as model-based approaches approximating likelihoods, posteriors, or density ratios. We contrast algorithms that use the prior distribution to propose parameters against ones that sequentially adapt the proposal. Keeping our initial selection of algorithms focused allowed us to carefully consider implementation details and hyperparameters. Metrics. The shortcomings of commonly used metrics (see paper for details) led us to focus on tasks for which a likelihood can be evaluated, which allowed us to calculate reference (\u2018ground-truth\u2019) posteriors. These reference posteriors are made available to allow rapid evaluation of SBI algorithms. While we compare algorithms in terms of classifier 2-sample tests (C2ST) in the paper, the website allows comparisons in terms of all metrics we considered. Tasks. We focused on eight purely statistical problems and two problems relevant in applied domains, with diverse dimensionalities of parameters and data. Key Findings The full potential of the benchmark will be realized when it is populated with additional community-contributed algorithms and tasks. However, our initial version already provides useful insights: the choice of performance metric is critical (commonly used metrics such as the likelihood of true parameters or Maximum Mean Discrepancy with median heuristic have important shortcomings); the performance of the algorithms on some tasks leaves substantial room for improvement; sequential estimation generally improves sample efficiency; for small and moderate simulation budgets, neural-network based approaches outperform classical ABC algorithms, confirming recent progress in the field; but that there is no algorithm to rule them all. The performance ranking of algorithms is task-dependent, pointing to a need for better guidance or automated procedures for choosing which algorithm to use when. In the manuscript, we included some considerations and recommendations for practitioners, based on our current results and understanding, and dedicated a page to discussing various limitations. Find out more in our manuscript, available through PMLR or: arXiv.org/abs/2101.04653 We believe that the full potential of the benchmark will be revealed as more researchers participate and contribute. In order to facilitate this process, we provide a benchmarking framework which is designed to be highly extensible and easily used. See Code & Reproducibility for explanations and examples. 3-Minute Summary embed = new SlidesLiveEmbed('presentation-embed-38952956', { presentationId: '38952956', autoPlay: false, verticalEnabled: true, zoomRatio: 0.3 }); Citation @InProceedings{lueckmann2021benchmarking, title = {Benchmarking Simulation-Based Inference}, author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob}, booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics}, pages = {343--351}, year = {2021}, editor = {Banerjee, Arindam and Fukumizu, Kenji}, volume = {130}, series = {Proceedings of Machine Learning Research}, month = {13--15 Apr}, publisher = {PMLR} } Support This work was supported by the German Research Foundation (DFG; SFB 1233 PN 276693517, SFB 1089, SPP 2041, Germany\u2019s Excellence Strategy \u2013 EXC number 2064/1 PN 390727645) and the German Federal Ministry of Education and Research (BMBF; project \u2019ADIMEM\u2019, FKZ 01IS18052 A-D). ADIMEM is a BMBF grant awarded to groups at the Technical University of Munich, University of T\u00fcbingen and Research Center caesar of the Max Planck Gesellschaft.","title":"Overview"},{"location":"#benchmarking-simulation-based-inference","text":"Many domains of science make use of numerical simulators for which (1) it is easy to simulate but (2) the likelihood is unavailable, making it hard to perform statistical identification of model parameters consistent with observed data. Simulation-based inference (SBI) deals with this 'likelihood-free' setting. Although recent advances have led to a large number of SBI algorithms, a public benchmark for such algorithms has been lacking: We set out to fill this gap, carefully select tasks and metrics, and evaluate several canonical algorithms. Through this website you can explore all results of our manuscript , including comparisons on all metrics and plotting of posteriors for each of more than 10 000 runs. Keep reading for a brief summary of the manuscript, or jump right into interactive results (through the menu on top). We provide a framework to benchmark your own algorithms, code and results for reproducibility, and invite contributions.","title":"Benchmarking Simulation-Based Inference"},{"location":"#our-motivation-and-methods","text":"Open benchmarks can be an important component of transparent and reproducible computational research. However, a benchmark framework for SBI has been lacking, possibly due to the challenging endeavour of designing benchmarking tasks and defining suitable performance metrics. We selected a set of initial algorithms representing four distinct approaches to SBI, analyzed multiple performance metrics which have been used in the literature, and implemented ten tasks, including ones popular in the field. Overview of algorithms. Classification and schemes following Cranmer et al. (2020) . Algorithms . We compare algorithms belonging to four distinct approaches to SBI: Classical ABC approaches as well as model-based approaches approximating likelihoods, posteriors, or density ratios. We contrast algorithms that use the prior distribution to propose parameters against ones that sequentially adapt the proposal. Keeping our initial selection of algorithms focused allowed us to carefully consider implementation details and hyperparameters. Metrics. The shortcomings of commonly used metrics (see paper for details) led us to focus on tasks for which a likelihood can be evaluated, which allowed us to calculate reference (\u2018ground-truth\u2019) posteriors. These reference posteriors are made available to allow rapid evaluation of SBI algorithms. While we compare algorithms in terms of classifier 2-sample tests (C2ST) in the paper, the website allows comparisons in terms of all metrics we considered. Tasks. We focused on eight purely statistical problems and two problems relevant in applied domains, with diverse dimensionalities of parameters and data.","title":"Our Motivation and Methods"},{"location":"#key-findings","text":"The full potential of the benchmark will be realized when it is populated with additional community-contributed algorithms and tasks. However, our initial version already provides useful insights: the choice of performance metric is critical (commonly used metrics such as the likelihood of true parameters or Maximum Mean Discrepancy with median heuristic have important shortcomings); the performance of the algorithms on some tasks leaves substantial room for improvement; sequential estimation generally improves sample efficiency; for small and moderate simulation budgets, neural-network based approaches outperform classical ABC algorithms, confirming recent progress in the field; but that there is no algorithm to rule them all. The performance ranking of algorithms is task-dependent, pointing to a need for better guidance or automated procedures for choosing which algorithm to use when. In the manuscript, we included some considerations and recommendations for practitioners, based on our current results and understanding, and dedicated a page to discussing various limitations. Find out more in our manuscript, available through PMLR or: arXiv.org/abs/2101.04653 We believe that the full potential of the benchmark will be revealed as more researchers participate and contribute. In order to facilitate this process, we provide a benchmarking framework which is designed to be highly extensible and easily used. See Code & Reproducibility for explanations and examples.","title":"Key Findings"},{"location":"#3-minute-summary","text":"embed = new SlidesLiveEmbed('presentation-embed-38952956', { presentationId: '38952956', autoPlay: false, verticalEnabled: true, zoomRatio: 0.3 });","title":"3-Minute Summary"},{"location":"#citation","text":"@InProceedings{lueckmann2021benchmarking, title = {Benchmarking Simulation-Based Inference}, author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob}, booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics}, pages = {343--351}, year = {2021}, editor = {Banerjee, Arindam and Fukumizu, Kenji}, volume = {130}, series = {Proceedings of Machine Learning Research}, month = {13--15 Apr}, publisher = {PMLR} }","title":"Citation"},{"location":"#support","text":"This work was supported by the German Research Foundation (DFG; SFB 1233 PN 276693517, SFB 1089, SPP 2041, Germany\u2019s Excellence Strategy \u2013 EXC number 2064/1 PN 390727645) and the German Federal Ministry of Education and Research (BMBF; project \u2019ADIMEM\u2019, FKZ 01IS18052 A-D). ADIMEM is a BMBF grant awarded to groups at the Technical University of Munich, University of T\u00fcbingen and Research Center caesar of the Max Planck Gesellschaft.","title":"Support"},{"location":"code/","text":"Code and Reproducibility How to benchmark new algorithms? We provide sbibm , a simulation-based inference benchmarking framework, which is available at: github.com/sbi-benchmark/sbibm sbibm includes tasks, reference posteriors, metrics, plotting, and integrations with exiting algorithm toolboxes. It is designed to be highly extensible and easily used in research projects: Having installed the framework via pip install sbibm , using it to benchmark existing or new algorithms is very simple, as we sketch out in the example code below: import sbibm task = sbibm.get_task(\"two_moons\") # See sbibm.get_available_tasks() for all tasks prior = task.get_prior() simulator = task.get_simulator() observation = task.get_observation(num_observation=1) # 10 per task # These objects can then be used for custom inference algorithms, e.g. # we might want to generate simulations by sampling from prior: thetas = prior(num_samples=10_000) xs = simulator(thetas) # Alternatively, we can import existing algorithms, e.g: from sbibm.algorithms import rej_abc # See help(rej_abc) for keywords posterior_samples, _, _ = rej_abc(task=task, num_samples=10_000, num_observation=1, num_simulations=100_000) # Once we got samples from an approximate posterior, compare them to the reference: from sbibm.metrics import c2st reference_samples = task.get_reference_posterior_samples(num_observation=1) c2st_accuracy = c2st(reference_samples, posterior_samples) # Visualise both posteriors: from sbibm.visualisation import fig_posterior fig = fig_posterior(task_name=\"two_moons\", observation=1, samples=[posterior_samples]) # Note: Use fig.show() or fig.save() to show or save the figure # Get results from other algorithms for comparison: from sbibm.visualisation import fig_metric results_df = sbibm.get_results(dataset=\"main_paper.csv\") fig = fig_metric(results_df.query(\"task == 'two_moons'\"), metric=\"C2ST\") More details can be found in the sbibm repository . Manuscript results All results of the manuscript, as well as code and instructions to reproduce them, are available at: github.com/sbi-benchmark/results/benchmarking_sbi","title":"Code & Reproducibility"},{"location":"code/#code-and-reproducibility","text":"","title":"Code and Reproducibility"},{"location":"code/#how-to-benchmark-new-algorithms","text":"We provide sbibm , a simulation-based inference benchmarking framework, which is available at: github.com/sbi-benchmark/sbibm sbibm includes tasks, reference posteriors, metrics, plotting, and integrations with exiting algorithm toolboxes. It is designed to be highly extensible and easily used in research projects: Having installed the framework via pip install sbibm , using it to benchmark existing or new algorithms is very simple, as we sketch out in the example code below: import sbibm task = sbibm.get_task(\"two_moons\") # See sbibm.get_available_tasks() for all tasks prior = task.get_prior() simulator = task.get_simulator() observation = task.get_observation(num_observation=1) # 10 per task # These objects can then be used for custom inference algorithms, e.g. # we might want to generate simulations by sampling from prior: thetas = prior(num_samples=10_000) xs = simulator(thetas) # Alternatively, we can import existing algorithms, e.g: from sbibm.algorithms import rej_abc # See help(rej_abc) for keywords posterior_samples, _, _ = rej_abc(task=task, num_samples=10_000, num_observation=1, num_simulations=100_000) # Once we got samples from an approximate posterior, compare them to the reference: from sbibm.metrics import c2st reference_samples = task.get_reference_posterior_samples(num_observation=1) c2st_accuracy = c2st(reference_samples, posterior_samples) # Visualise both posteriors: from sbibm.visualisation import fig_posterior fig = fig_posterior(task_name=\"two_moons\", observation=1, samples=[posterior_samples]) # Note: Use fig.show() or fig.save() to show or save the figure # Get results from other algorithms for comparison: from sbibm.visualisation import fig_metric results_df = sbibm.get_results(dataset=\"main_paper.csv\") fig = fig_metric(results_df.query(\"task == 'two_moons'\"), metric=\"C2ST\") More details can be found in the sbibm repository .","title":"How to benchmark new algorithms?"},{"location":"code/#manuscript-results","text":"All results of the manuscript, as well as code and instructions to reproduce them, are available at: github.com/sbi-benchmark/results/benchmarking_sbi","title":"Manuscript results"},{"location":"contribute/","text":"Contribute The full potential of the benchmark will be realized when it is populated with additional algorithms, tasks, and results. We invite contributions and hope to inspire the community to participate in this endeavour. To facilitate this process, we provide a sbibm , an extensible framework for benchmarking (see Code & Reproducibility ). We have put together some notes for contributions to sbibm : https://github.com/sbi-benchmark/sbibm/blob/main/CONTRIBUTING.md Please do not hestitate to get in touch via email or by opening an issue on the repository.","title":"Contributions"},{"location":"contribute/#contribute","text":"The full potential of the benchmark will be realized when it is populated with additional algorithms, tasks, and results. We invite contributions and hope to inspire the community to participate in this endeavour. To facilitate this process, we provide a sbibm , an extensible framework for benchmarking (see Code & Reproducibility ). We have put together some notes for contributions to sbibm : https://github.com/sbi-benchmark/sbibm/blob/main/CONTRIBUTING.md Please do not hestitate to get in touch via email or by opening an issue on the repository.","title":"Contribute"},{"location":"results/correlations/","text":"","title":"Correlations"},{"location":"results/download/","text":"Dataframes ...","title":"Dataframes"},{"location":"results/download/#dataframes","text":"...","title":"Dataframes"},{"location":"results/hiplot/","text":"","title":"Hiplot"},{"location":"results/metrics/","text":"","title":"Metrics"},{"location":"results/offline/","text":"Offline The server hosting the interactive results is unfortunately offline at the moment. We hope it will be back online soon, for updates follow @janmatthis .","title":"Offline"},{"location":"results/offline/#offline","text":"The server hosting the interactive results is unfortunately offline at the moment. We hope it will be back online soon, for updates follow @janmatthis .","title":"Offline"},{"location":"results/posteriors/","text":"","title":"Posteriors"}]}